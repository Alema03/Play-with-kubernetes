# Kubernetes para principiantes

## Introducci√≥n
En este taller pr√°ctico, aprender√° los conceptos b√°sicos de Kubernetes. Lo har√° interactuando con Kubernetes a trav√©s de las terminales de l√≠nea de comandos que se muestran a la derecha. Finalmente, implementar√° las aplicaciones de muestra Dockercoins en ambos nodos de trabajo.

## Empezando
¬øQu√© son estos terminales en los navegadores?
A la derecha, deber√≠as ver dos ventanas de terminal. Es posible que se te solicite que inicies sesi√≥n primero, lo que puedes hacer con un ID de Docker o una cuenta de GitHub . Esas terminales son terminales completamente funcionales que ejecutan Centos. En realidad, son la l√≠nea de comandos para los contenedores de Centos que se ejecutan en nuestra infraestructura Play-with-Kubernetes. Cuando veas bloques de c√≥digo que se vean as√≠, puedes hacer clic en ellos y se completar√°n autom√°ticamente, o puedes copiarlos y pegarlos.

 ```ls```
![alt text](image.png)
## Iniciar el cluster
El primer paso es inicializar el cl√∫ster en la primera terminal:

  ```kubeadm init --apiserver-advertise-address $(hostname -i)```
  ![alt text](image-1.png)

Esto tomar√° un par de minutos, durante los cuales ver√° mucha actividad en la terminal.

Ver√°s algo como esto al final:

![alt text](image-2.png)

Copia toda la l√≠nea que comienza kubeadm joinen la primera terminal y p√©gala en la segunda. Deber√≠as ver algo como esto:


![alt text](image-3.png)
Eso significa que ya casi est√°s listo para empezar. Por √∫ltimo, solo tienes que inicializar la red del cl√∫ster en la primera terminal:

   ```kubectl apply -n kube-system -f \
    "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 |tr -d '\n')"
  ```

Ver√°s un resultado como este:

![alt text](image-4.png)

¬°Tu cluster est√° configurado!

## ¬øQu√© es esta aplicaci√≥n?
- ¬°Es un minero de DockerCoin! üí∞üê≥üì¶üö¢

- No, no puedes comprar caf√© con DockerCoins

- C√≥mo funcionan las DockerCoins:

  - workerpide rnggenerar algunos bytes aleatorios

  - workerintroduce estos bytes enhasher

  - ¬°y repetir para siempre!

  - Cada segundo, workerse actualiza redispara indicar cu√°ntos bucles se realizaron.

  - webuiconsultas redis, calcula y expone la "velocidad de hash" en su navegador

## Obtener el c√≥digo fuente de la aplicaci√≥n
Hemos creado una aplicaci√≥n de muestra para ejecutar en partes del taller. La aplicaci√≥n se encuentra en el repositorio de dockercoins .

Veamos el dise√±o general del c√≥digo fuente:

Hay un archivo Compose ```docker-compose.yml‚Ä¶```

‚Ä¶y otros 4 servicios, cada uno en su propio directorio:

```rng```= servicio web que genera bytes aleatorios

```hasher```= servicio web que calcula el hash de los datos publicados

```worker```= proceso en segundo plano que utiliza rngyhasher

```webui```= Interfaz web para ver el progreso.

- Clonaremos el repositorio de GitHub

![alt text](image-5.png)

- El repositorio tambi√©n contiene scripts y herramientas que usaremos durante el taller.

  ```git clone https://github.com/dockersamples/dockercoins```

  ![alt text](image-6.png)

(Tambi√©n puedes bifurcar el repositorio en GitHub y clonar tu bifurcaci√≥n si lo prefieres).

## Ejecutando la aplicaci√≥n
- Vaya al directorio dockercoins, en el repositorio clonado:

  ```cd ~/dockercoins```

  ![alt text](image-7.png)

- Utilice Compose para crear y ejecutar todos los contenedores:

  ```docker-compose up```
  ![alt text](image-8.png)

Compose le dice a Docker que construya todas las im√°genes de contenedores (extrayendo las im√°genes base correspondientes), luego inicia todos los contenedores y muestra registros agregados.

Muchos registros
- La aplicaci√≥n genera registros continuamente

- Podemos ver el servicio de trabajo haciendo solicitudes a rng y hasher

- Dejemos eso en segundo plano.

## Conexi√≥n a la interfaz web
- El webui contenedor expone un panel web; ve√°moslo

- Con un navegador web, con√©ctese node1 aqu√≠ en el puerto 8000 (creado cuando ejecut√≥ la aplicaci√≥n)

  ![alt text](image-9.png)

## Limpiar
- Antes de continuar, apaguemos todo escribiendo Ctrl-C.
  ![alt text](image-10.png)
## Conceptos de Kubernetes
- Kubernetes es un sistema de gesti√≥n de contenedores

- Ejecuta y administra aplicaciones en contenedores en un cl√∫ster.

- ¬øQu√© significa eso realmente?

## Cosas b√°sicas que podemos pedirle a Kubernetes que haga
- Iniciar 5 contenedores usando la image natseashop/api:v1.3

- Coloque un balanceador de carga interno frente a estos contenedores

- Iniciar 10 contenedores usando la imagenatseashop/webfront:v1.3

- Coloque un balanceador de carga p√∫blico frente a estos contenedores

- Es Viernes Negro (o Navidad), picos de tr√°fico, hacemos crecer nuestro cl√∫ster y agregamos contenedores

- ¬°Nuevo lanzamiento! Reemplace mis contenedores con la nueva imagenatseashop/webfront:v1.4

- Seguir procesando solicitudes durante la actualizaci√≥n; actualizar mis contenedores uno a la vez

## Otras cosas que Kubernetes puede hacer por nosotros
- Escalado autom√°tico b√°sico

- Implementaci√≥n azul/verde, implementaci√≥n canaria

- Servicios de larga duraci√≥n, pero tambi√©n trabajos por lotes (√∫nicos)

- Sobrecargar nuestro cl√∫ster y desalojar trabajos de baja prioridad

- Ejecutar servicios con datos con estado (bases de datos, etc.)

- Control de acceso detallado que define qu√© puede hacer cada persona y en qu√© recursos

- Integraci√≥n de servicios de terceros (cat√°logo de servicios)

- Automatizaci√≥n de tareas complejas (operadores)

## Arquitectura de Kubernetes
### Arquitectura de Kubernetes: el maestro
- La l√≥gica de Kubernetes (su ‚Äúcerebro‚Äù) es una colecci√≥n de servicios:

  - el servidor API (¬°nuestro punto de entrada a todo!)
  - Servicios b√°sicos como el programador y el administrador del controlador.
  - etcd (un almac√©n de claves y valores de alta disponibilidad; la ‚Äúbase de datos‚Äù de Kubernetes)
*En conjunto, estos servicios forman lo que se denomina el ‚Äúmaestro‚Äù

- Estos servicios pueden ejecutarse directamente en un host o en contenedores (ese es un detalle de implementaci√≥n).

- etcd se puede ejecutar en m√°quinas separadas (primer esquema) o en el mismo lugar (segundo esquema)

- Necesitamos al menos un maestro, pero podemos tener m√°s (para alta disponibilidad)

## Arquitectura de Kubernetes: los nodos
- Los nodos que ejecutan nuestros contenedores ejecutan otra colecci√≥n de servicios:

  - Un motor de contenedores (normalmente Docker)
  - kubelet (el ‚Äúagente de nodo‚Äù)
  - kube-proxy (un componente de red necesario pero no suficiente)
- Los nodos antes se llamaban ‚Äúminions‚Äù

- Es habitual no ejecutar aplicaciones en los nodos que ejecutan componentes maestros (excepto cuando se utilizan peque√±os cl√∫steres de desarrollo)

## Recursos de Kubernetes
- La API de Kubernetes define una gran cantidad de objetos llamados recursos

- Estos recursos est√°n organizados por tipo o Kind (en la API)

- Algunos tipos de recursos comunes son:

  - nodo (una m√°quina, f√≠sica o virtual, en nuestro cl√∫ster)
  - pod (grupo de contenedores que se ejecutan juntos en un nodo)
  - servicio (punto final de red estable para conectarse a uno o varios contenedores)
  - espacio de nombres (grupo de cosas m√°s o menos aislado)
  - secreto (conjunto de datos confidenciales que se pasar√°n a un contenedor)
- ¬°Y mucho m√°s! (Podemos ver la lista completa ejecutando kubectl get)

## Declarativo vs imperativo
- Nuestro orquestador de contenedores pone mucho √©nfasis en ser declarativo

- Declarativo:

      me gustar√≠a una taza de t√©.

- Imperativo:

      Hierve un poco de agua. Vi√©rtela en una tetera. A√±ade hojas de t√©. Deja reposar un rato. Sirve en una taza.

- El declarativo parece m√°s simple al principio‚Ä¶

- ‚Ä¶ Siempre que sepas preparar t√©

### Lo que realmente ser√≠a declarativo:
Quiero una taza de t√©, obtenida vertiendo una infusi√≥n de hojas de t√© en una taza.

Se obtiene una infusi√≥n dejando el objeto en remojo unos minutos en agua caliente.

El l√≠quido caliente se obtiene verti√©ndolo en un recipiente apropiado y poni√©ndolo sobre el fuego.

¬°Ah, por fin, contenedores! Algo que ya conocemos. ¬°Manos a la obra!

## Resumen de declarativo vs imperativo
- Sistemas imperativos:

  - M√°s simple

  - Si una tarea se interrumpe, tenemos que reiniciarla desde cero

- Sistemas declarativos:

  - Si se interrumpe una tarea (o si llegamos a la fiesta a mitad de camino), podemos averiguar qu√© falta y hacer solo lo necesario.

  - Necesitamos poder observar el sistema

  - ‚Ä¶y calcular una ‚Äúdiferencia‚Äù entre lo que tenemos y lo que queremos

## Declarativo vs imperativo en Kubernetes
- Pr√°cticamente todo lo que creamos en Kubernetes se crea a partir de unspec

- ¬°Est√© atento a los speccampos en los archivos YAML m√°s tarde!

- El specdescribe c√≥mo queremos que sea la cosa.

- Kubernetes conciliar√° el estado actual con la especificaci√≥n (t√©cnicamente, esto lo hacen varios controladores )

- Cuando queremos cambiar alg√∫n recurso, actualizamos elspec

- Kubernetes luego converger√° ese recurso.

## Modelo de red de Kubernetes
- Resumen:

  - Nuestro cl√∫ster (nodos y pods) es una gran red IP plana.

- En detalle:

  - Todos los nodos deben poder comunicarse entre s√≠, sin NAT.

  - Todos los pods deben poder comunicarse entre s√≠, sin NAT

  - Los pods y los nodos deben poder comunicarse entre s√≠, sin NAT

  - Cada pod conoce su direcci√≥n IP (sin NAT)

- Kubernetes no exige ninguna implementaci√≥n en particular

## Modelo de red de Kubernetes: lo bueno
- Todo puede alcanzar todo.

- Sin traducci√≥n de direcci√≥n

- Sin traducci√≥n de puerto

- No hay nuevo protocolo

- Los pods no pueden moverse de un nodo a otro y mantener su direcci√≥n IP

- Las direcciones IP no tienen que ser ‚Äúportables‚Äù de un nodo a otro (podemos usar, por ejemplo, una subred por nodo y utilizar una topolog√≠a enrutada simple)

- La especificaci√≥n es lo suficientemente simple como para permitir muchas implementaciones diferentes.

## Modelo de red de Kubernetes: lo menos bueno
- Todo puede alcanzar todo.

  - Si quieres seguridad, necesitas agregar pol√≠ticas de red.

  - La implementaci√≥n de red que utilice debe admitirlos.

- Existen literalmente docenas de implementaciones (15 est√°n enumeradas en la documentaci√≥n de Kubernetes)

- Parece que tienes una red de nivel 3, pero solo es de nivel 4 (la especificaci√≥n requiere UDP y TCP, pero no rangos de puertos ni paquetes IP arbitrarios)

- kube-proxyest√° en la ruta de datos cuando se conecta a un pod o contenedor, y no es particularmente r√°pido (depende del proxy del espacio de usuario o de iptables)

## Modelo de red de Kubernetes: en la pr√°ctica
- Los nodos que estamos utilizando se han configurado para utilizar Weave

- No respaldamos a Weave de ninguna manera en particular, simplemente funciona para nosotros

- No te preocupes por la advertencia sobre el rendimiento de kube-proxy

- A menos que usted:

  - saturar rutinariamente las interfaces de red 10G

  - contar las tasas de paquetes en millones por segundo

  - Ejecutar plataformas de juegos o VOIP de alto tr√°fico

  - hacer cosas raras que involucran millones de conexiones simult√°neas (en cuyo caso ya est√°s familiarizado con el ajuste del kernel)

## Primer contacto conkubectl
- kubectles (casi) la √∫nica herramienta que necesitaremos para comunicarnos con Kubernetes

- Es una herramienta CLI rica en torno a la API de Kubernetes (todo lo que puede hacer con kubectl, puede hacerlo directamente con la API)

- Tambi√©n puedes usar la --kubeconfigbandera para pasar un archivo de configuraci√≥n

- O directamente --server, --user, etc.

- kubectlse puede pronunciar ‚ÄúCube CTL‚Äù, ‚ÄúCube cuttle‚Äù, ‚ÄúCube cuddle‚Äù‚Ä¶

## kubectl get
- ¬°Veamos nuestros recursos de Node con kubectl get!

- Observa la composici√≥n de nuestro cluster:

  ```kubectl get node```

  ![alt text](image-11.png)


- Estos comandos son equivalentes
  ```
  kubectl get no
  kubectl get node
  kubectl get nodes
  ```
  ![alt text](image-12.png)
## Obtenci√≥n de una salida legible por m√°quina
- kubectl getPuede generar JSON, YAML o formatearse directamente.

- Danos m√°s informaci√≥n sobre los nodos:

  ```kubectl get nodes -o wide```
  ![alt text](image-13.png)

- Vamos a tener algo de YAML:

  ```kubectl get no -o yaml```
  ![alt text](image-14.png)

   ¬øVes ese tipo de lista al final? ¬°Es el tipo de nuestro resultado!

## (Ab)usar kubectlyjq
Es muy f√°cil crear informes personalizados

Muestra la capacidad de todos nuestros nodos como un flujo de objetos JSON:

kubectl get nodes -o json |
      jq ".items[] | {name:.metadata.name} + .status.capacity"
## ¬øQue hay disponible?
kubectlTiene muy buenas instalaciones de introspecci√≥n.

Podemos enumerar todos los tipos de recursos disponibles ejecutandokubectl get

Podemos ver detalles sobre un recurso con:
kubectl describe type/name
kubectl describe type name
Podemos ver la definici√≥n de un tipo de recurso con:
kubectl explain type
Cada vez, typeel nombre puede ser singular, plural o abreviado.

## Servicios
Un servicio es un punto final estable para conectarse a ‚Äúalgo‚Äù (en la propuesta inicial se llamaban ‚Äúportales‚Äù)

Enumere los servicios en nuestro cl√∫ster:

kubectl get services
Esto tambi√©n funcionar√≠a:

    kubectl get svc
Ya hay un servicio en nuestro cl√∫ster: la propia API de Kubernetes.

## Servicios de ClusterIP
Un ClusterIPservicio es interno y solo est√° disponible desde el cl√∫ster.

Esto es √∫til para la introspecci√≥n desde dentro de los contenedores.

Intente conectarse a la API.

-kse utiliza para omitir la verificaci√≥n del certificado
Aseg√∫rese de reemplazar 10.96.0.1 con el CLUSTER-IP que se muestra en$ kubectl get svc
curl -k https://10.96.0.1
El error que vemos es el esperado: la API de Kubernetes requiere autenticaci√≥n.

## Listado de contenedores en funcionamiento
Los contenedores se manipulan a trav√©s de pods.

Un pod es un grupo de contenedores:

corriendo juntos (en el mismo nodo)

compartir recursos (RAM, CPU; pero tambi√©n red, vol√∫menes)

Lista de pods en nuestro cl√∫ster:

kubectl get pods
Estas no son las c√°psulas que est√°s buscando . ¬øPero d√≥nde est√°n?

## Espacios de nombres
Los espacios de nombres nos permiten segregar recursos

Enumere los espacios de nombres en nuestro cl√∫ster con uno de estos comandos:

kubectl get namespaces
Cualquiera de estos tambi√©n funcionar√≠a:

kubectl get namespace
kubectl get ns
¬øSabes qu√©? Esta kube-systemcosa parece sospechosa.

## Acceder a espacios de nombres
De forma predeterminada, kubectlutiliza el defaultespacio de nombres

Podemos cambiar a un espacio de nombres diferente con la -nopci√≥n

Enumere los pods en el kube-systemespacio de nombres:

kubectl -n kube-system get pods
¬°Ding, ding, ding, ding!

## ¬øQu√© son todas estas vainas?
etcd¬øEs nuestro servidor etcd?

kube-apiserveres el servidor API

kube-controller-managery kube-schedulerson otros componentes maestros

kube-dnses un componente adicional (no obligatorio pero s√∫per √∫til, por eso est√° ah√≠)

kube-proxyes el componente (por nodo) que administra las asignaciones de puertos y dem√°s

weavees el componente (por nodo) que administra la superposici√≥n de la red
La READYcolumna indica el n√∫mero de contenedores en cada pod.

Los pods con un nombre que termina en -node1son los componentes maestros (han sido espec√≠ficamente "anclados" al nodo maestro)

## Ejecutando nuestros primeros contenedores en Kubernetes
Lo primero es lo primero: no podemos ejecutar un contenedor

Vamos a ejecutar un pod, y en ese pod habr√° un solo contenedor.

En ese contenedor del pod, vamos a ejecutar un comando ping simple

Luego vamos a iniciar copias adicionales del pod.

## Iniciando un pod simple conkubectl run
Necesitamos especificar al menos un nombre y la imagen que queremos utilizar.

Hagamos ping al 8.8.8.8DNS p√∫blico de Google

kubectl run pingpong --image alpine ping 8.8.8.8
Bueno, ¬øqu√© acaba de pasar?

## Entre bastidores dekubectl run
Veamos los recursos que fueron creados porkubectl run

Enumere la mayor√≠a de los tipos de recursos:

kubectl get all
Deber√≠amos ver las siguientes cosas:

deploy/pingpong(la implementaci√≥n que acabamos de crear)
rs/pingpong-xxxx(un conjunto de r√©plicas creado por la implementaci√≥n)
po/pingpong-yyyy(una c√°psula creada por el conjunto de r√©plicas)
## ¬øQu√© son estas cosas diferentes?
Una implementaci√≥n es una construcci√≥n de alto nivel

Permite escalar, realizar actualizaciones y reversiones.

Se pueden usar varias implementaciones juntas para implementar una implementaci√≥n canaria

delega la gesti√≥n de pods a conjuntos de r√©plicas

Un conjunto de r√©plicas es una construcci√≥n de bajo nivel.

se asegura de que se est√© ejecutando una cantidad determinada de pods id√©nticos

permite escalar

rara vez se utiliza directamente

Un controlador de replicaci√≥n es el predecesor (obsoleto) de un conjunto de r√©plicas

## Nuestro pingpongdespliegue
kubectl runcre√≥ una implementaci√≥n ,deploy/pingpong

Esa implementaci√≥n cre√≥ un conjunto de r√©plicas ,rs/pingpong-xxxx

Ese conjunto de r√©plicas cre√≥ una c√°psula ,po/pingpong-yyyy

Veremos m√°s adelante c√≥mo juegan juntos estos chicos:

escalada

alta disponibilidad

actualizaciones continuas

## Visualizaci√≥n de la salida del contenedor
Vamos a utilizar el kubectl logscomando

Pasaremos un nombre de pod o un tipo/nombre (por ejemplo, si especificamos un conjunto de implementaci√≥n o r√©plica, obtendr√° el primer pod que contenga).

A menos que se especifique lo contrario, solo se mostrar√°n los registros del primer contenedor en el pod (¬°menos mal que solo hay uno en el nuestro!).

Vea el resultado de nuestro comando ping:

kubectl logs deploy/pingpong
## Registros de transmisi√≥n en tiempo real
Al igual que docker logs, kubectl logsadmite opciones convenientes:

-f/--followpara transmitir registros en tiempo real (a la tail -f)

--tailpara indicar cu√°ntas l√≠neas quieres ver (desde el final)

--sincePara obtener registros solo despu√©s de una marca de tiempo determinada

Ver los √∫ltimos registros de nuestro comando ping:

kubectl logs deploy/pingpong --tail 1 --follow
## Escalando nuestra aplicaci√≥n
Podemos crear copias adicionales de nuestro contenedor (o m√°s bien de nuestro pod) conkubectl scale

Escale nuestra implementaci√≥n de pingpong:

kubectl scale deploy/pingpong --replicas 8
Nota: ¬øQu√© suceder√≠a si intent√°ramos escalar rs/pingpong-xxxx? ¬°Podr√≠amos! Pero la implementaci√≥n lo detectar√≠a de inmediato y volver√≠a al nivel inicial.

## Resiliencia
El pingpong de despliegue vigila su conjunto de r√©plicas

El conjunto de r√©plicas garantiza que se est√© ejecutando la cantidad correcta de pods

## ¬øQu√© pasa si las vainas desaparecen?

En una ventana separada, enumera los pods y contin√∫a observ√°ndolos:
kubectl get pods -w
Ctrl-Cpara terminar de mirar.

Si quisieras destruir un pod, usar√≠as este patr√≥n donde yyyyestaba el identificador del pod en particular:
kubectl delete pod pingpong-yyyy
¬øQu√© pasar√≠a si quisi√©ramos algo diferente?
¬øQu√© pasa si queremos iniciar un contenedor ‚Äúone-shot‚Äù que no se reinicia?

Podr√≠amos utilizar kubectl run --restart=OnFailureokubectl run --restart=Never

Estos comandos crear√≠an trabajos o pods en lugar de implementaciones.

Bajo el cap√≥, kubectl runinvoca ‚Äúgeneradores‚Äù para crear descripciones de recursos

Tambi√©n podr√≠amos escribir estas descripciones de recursos nosotros mismos (normalmente en YAML) y crearlas en el cl√∫ster con kubectl apply -f(se analiza m√°s adelante)

Con kubectl run --schedule=‚Ä¶, tambi√©n podemos crear cronjobs

## Visualizaci√≥n de registros de varios pods
Cuando especificamos un nombre de implementaci√≥n, solo se muestran los registros de un solo pod

Podemos ver los registros de varios pods especificando un selector

Un selector es una expresi√≥n l√≥gica que utiliza etiquetas.

Convenientemente, cuando you kubectl run somename, los objetos asociados tienen una run=somenameetiqueta

Ver la √∫ltima l√≠nea del registro de todos los pods con la run=pingpongetiqueta:

kubectl logs -l run=pingpong --tail 1
Lamentablemente, --followno se puede usar (todav√≠a) para transmitir los registros desde m√∫ltiples contenedores.

## Limpiar
Limpia tu implementaci√≥n eliminandopingpong

kubectl delete deploy/pingpong
## Exponiendo contenedores
kubectl exposeCrea un servicio para pods existentes.

Un servicio es una direcci√≥n estable para un pod (o un grupo de pods)

Si queremos conectarnos a nuestro(s) pod(es), necesitamos crear un servicio

Una vez que se crea un servicio, kube-dnsnos permitir√° resolverlo por nombre (es decir, despu√©s de crear el servicio hello, el nombre hellose resolver√° en algo)

Existen diferentes tipos de servicios, detallados en las siguientes diapositivas:

ClusterIP, NodePort, LoadBalancer,ExternalName

Tipos de servicios b√°sicos
ClusterIP(tipo predeterminado)

Se asigna una direcci√≥n IP virtual para el servicio (en un rango interno privado). Esta direcci√≥n IP solo es accesible desde dentro del cl√∫ster (nodos y pods). Nuestro c√≥digo puede conectarse al servicio usando el n√∫mero de puerto original.

NodePort

se asigna un puerto para el servicio (por defecto, en el rango 30000-32768) ese puerto est√° disponible en todos nuestros nodos y cualquiera puede conectarse a √©l nuestro c√≥digo debe cambiarse para conectarse a ese nuevo n√∫mero de puerto

Estos tipos de servicios est√°n siempre disponibles.

Bajo el cap√≥: kube-proxyse utiliza un proxy de usuario y un mont√≥n de iptablesreglas.

## M√°s tipos de servicios
LoadBalancer
Se asigna un balanceador de carga externo para el servicio
El balanceador de carga se configura en consecuencia (por ejemplo: NodePortse crea un servicio y el balanceador de carga env√≠a tr√°fico a ese puerto)
ExternalName

La entrada DNS administrada por kube-dnsser√° solo una CNAMEpara un registro proporcionado
No se asigna ning√∫n puerto, ninguna direcci√≥n IP ni nada m√°s
## Ejecuci√≥n de contenedores con puertos abiertos
Dado que ping no tiene nada a lo que conectarse, tendremos que ejecutar algo m√°s.

Inicie un grupo de contenedores ElasticSearch:
kubectl run elastic --image=elasticsearch:2 --replicas=4
Mira como empiezan:

kubectl get pods -w
La -wopci√≥n ‚Äúobserva‚Äù los eventos que suceden en los recursos especificados.

Nota: NO llame al servicio search. Esto podr√≠a entrar en conflicto con el TLD.

## Exponiendo nuestro despliegue
Crearemos un ClusterIPservicio predeterminado

Exponer el puerto API HTTP de ElasticSearch:

kubectl expose deploy/elastic --port 9200
Busque qu√© direcci√≥n IP fue asignada:

kubectl get svc
## Los servicios son construcciones de capa 4
Puedes asignar direcciones IP a los servicios, pero siguen siendo de capa 4 (es decir, un servicio no es una direcci√≥n IP; es una direcci√≥n IP + protocolo + puerto).

Esto se debe a la implementaci√≥n actual de kube-proxy(se basa en mecanismos que no admiten la capa 3)

Como resultado: debes indicar el n√∫mero de puerto para tu servicio

La ejecuci√≥n de servicios con un puerto arbitrario (o rangos de puertos) requiere modificaciones (por ejemplo, el modo de red del host)

## Probando nuestro servicio
Ahora enviaremos algunas solicitudes HTTP a nuestros pods ElasticSearch

Obtengamos la direcci√≥n IP que fue asignada para nuestro servicio, program√°ticamente :

IP=$(kubectl get svc elastic -o go-template --template '{{ .spec.clusterIP }}')
Env√≠e algunas solicitudes:

curl http://$IP:9200/
Nuestras solicitudes se equilibran entre m√∫ltiples pods.

## Limpiar
Hemos terminado con la elasticimplementaci√≥n, as√≠ que vamos a limpiarla.

kubectl delete deploy/elastic
# Nuestra aplicaci√≥n en Kube
## ¬øQue hay en el menu?
En esta parte:

Crea im√°genes para nuestra aplicaci√≥n,

Env√≠a estas im√°genes con un registro,

ejecutar implementaciones utilizando estas im√°genes,

Exponer estas implementaciones para que puedan comunicarse entre s√≠,

Exponer la interfaz web para que podamos acceder a ella desde el exterior.

## El plan
Construir sobre nuestro nodo de control ( node1)

Etiqueta las im√°genes para que tengan nombre$USERNAME/servicename

Subirlos a un Docker Hub

Crear implementaciones utilizando las im√°genes

Exponer (con un ClusterIP) los servicios que necesitan comunicarse

Exponer (con un NodePort) la interfaz web

## Configuraci√≥n
En la primera terminal, configure una variable de entorno para su nombre de usuario de Docker Hub . Puede ser el mismo nombre de usuario de Docker Hub que utiliz√≥ para iniciar sesi√≥n en las terminales de este sitio.

export USERNAME=YourUserName
Aseg√∫rate de que todav√≠a est√°s en el dockercoinsdirectorio.

pwd
## Una nota sobre los registros
Para este taller, utilizaremos Docker Hub . Hay otras opciones, incluidas dos proporcionadas por Docker.

Docker tambi√©n proporciona:

Docker Trusted Registry , que agrega muchas funciones de seguridad e implementaci√≥n, incluido escaneo de seguridad y control de acceso basado en roles.
Registro de c√≥digo abierto de Docker .
## Centro Docker
Docker Hub es el registro predeterminado para Docker.

Los nombres de las im√°genes en Hub son simplemente $USERNAME/$IMAGENAMEo $ORGANIZATIONNAME/$IMAGENAME.

Las im√°genes oficiales se pueden denominar simplemente $IMAGENAME.

Para utilizar Hub, aseg√∫rese de tener una cuenta. Luego, ingrese docker logina la terminal e inicie sesi√≥n con su nombre de usuario y contrase√±a.

Al utilizar Docker Trusted Registry, Docker Open Source Registry es muy similar.

Los nombres de im√°genes en otros registros son $REGISTRYPATH/$USERNAME/$IMAGENAMEo $REGISTRYPATH/$ORGANIZATIONNAME/$IMAGENAME.

Iniciar sesi√≥n usando docker login $REGISTRYPATH.

## Construyendo y potenciando nuestras im√°genes
Vamos a utilizar una caracter√≠stica conveniente de Docker Compose

Ir al stacksdirectorio:

cd ~/dockercoins/stacks
Construye y env√≠a las im√°genes:

docker-compose -f dockercoins.yml build
docker-compose -f dockercoins.yml push
Echemos un vistazo al archivo dockercoins.yml mientras se compila y se env√≠a.

version: "3"
services:
  rng:
    build: dockercoins/rng
    image: ${USERNAME}/rng:${TAG-latest}
    deploy:
      mode: global
  ...
  redis:
    image: redis
  ...
  worker:
    build: dockercoins/worker
    image: ${USERNAME}/worker:${TAG-latest}
    ...
    deploy:
      replicas: 10
Por si acaso te lo estabas preguntando‚Ä¶ Los ‚Äúservicios‚Äù de Docker no son ‚Äúservicios‚Äù de Kubernetes.

## Desplegando todas las cosas
Ahora podemos implementar nuestro c√≥digo (as√≠ como una instancia de redis)

Desplegar redis:

kubectl run redis --image=redis
Implementar todo lo dem√°s:

for SERVICE in hasher rng webui worker; do
  kubectl run $SERVICE --image=$USERNAME/$SERVICE -l app=$SERVICE
done
## ¬øEsto funciona?
Despu√©s de esperar a que se complete la implementaci√≥n, ¬°veamos los registros!

(Sugerencia: √∫selo kubectl get deploy -wpara observar eventos de implementaci√≥n)

Mira algunos registros:

kubectl logs deploy/rng
kubectl logs deploy/worker
ü§î rngest√° bien‚Ä¶ Pero no worker.

üí° ¬°Ah, cierto! Nos olvidamos de expose.

# Servicios de exposici√≥n
## Exponer servicios internamente
Tres implementaciones deben ser accesibles para otros: hasher, redis,rng

workerNo necesita ser expuesto

webuiSe tratar√° m√°s adelante

Exponer cada implementaci√≥n, especificando el puerto correcto:

kubectl expose deployment redis --port 6379
kubectl expose deployment rng --port 80
kubectl expose deployment hasher --port 80
## ¬øEsto ya funciona?
Tiene workerun bucle infinito, que vuelve a intentarlo 10 segundos despu√©s de un error.

Transmitir los registros del trabajador:

kubectl logs deploy/worker --follow
(Dale unos 10 segundos para que se recupere)

Ahora deber√≠amos verlo worker, bueno, funcionando felizmente.
## Exponer servicios para acceso externo
Ahora nos gustar√≠a acceder a la interfaz web.

Lo expondremos con un NodePort(tal como lo hicimos para el registro)

Crear un NodePortservicio para la interfaz web:

kubectl create service nodeport webui --tcp=80 --node-port=30001
Verifique el puerto que fue asignado:

kubectl get svc
## Acceder a la interfaz web
Ahora podemos conectarnos a cualquier nodo , en el puerto de nodo asignado, para ver la interfaz de usuario web.
Haga clic en este enlace

¬°Muy bien, volvemos al punto de partida, cuando est√°bamos funcionando en un solo nodo!

# Implicaciones de seguridad dekubectl apply
Cuando lo hacemos kubectl apply -f <URL>, creamos recursos arbitrarios.

Los recursos pueden ser malos; imagina deploymenteso‚Ä¶

inicia mineros de bitcoin en todo el cl√∫ster

se oculta en un espacio de nombres no predeterminado

bind monta el sistema de archivos de nuestros nodos

inserta claves SSH en la cuenta ra√≠z (en el nodo)

Encripta nuestros datos y pide rescate por ellos

‚ò†Ô∏è‚ò†Ô∏è‚ò†Ô∏è

## kubectl applyEs el nuevocurl | sh
curl | shes conveniente

Es seguro si utiliza URL HTTPS de fuentes confiables

kubectl apply -fes conveniente

Es seguro si utiliza URL HTTPS de fuentes confiables

Introduce nuevos modos de fallo

Ejemplo: las instrucciones de configuraci√≥n oficiales para la mayor√≠a de las redes de pods

## Escalar una implementaci√≥n
Comenzaremos con uno f√°cil: el workerdespliegue.

kubectl get pods

kubectl get deployments
Ahora, crea m√°s workerr√©plicas:

kubectl scale deploy/worker --replicas=10
Despu√©s de unos segundos, deber√≠a aparecer el gr√°fico en la interfaz web (y alcanzar un m√°ximo de 10 hashes por segundo, tal como cuando est√°bamos trabajando en uno solo).

## Conjuntos de demonios
¬øQu√© pasa si queremos una (y exactamente una) instancia de rng por nodo?

Si simplemente escalamos la implementaci√≥n/rng a 2, nada garantiza que se propaguen.

En lugar de una implementaci√≥n, utilizaremos un daemonset

Los conjuntos de demonios son excelentes para procesos por nodo y a nivel de cl√∫ster:

proxy kube
tejido (nuestra red superpuesta)
agentes de vigilancia
herramientas de gesti√≥n de hardware (por ejemplo, agentes HBA SCSI/FC)
etc.
Tambi√©n se pueden restringir para que se ejecuten s√≥lo en algunos nodos .

## Creando un conjunto de demonios
Lamentablemente, a partir de Kubernetes 1.9, la CLI no puede crear conjuntos de demonios

M√°s precisamente: no tiene un subcomando para crear un conjunto de demonios.

Pero siempre se puede crear cualquier tipo de recurso proporcionando una descripci√≥n YAML:

kubectl apply -f foo.yaml
¬øC√≥mo creamos el archivo YAML para nuestro conjunto de demonios?

Opci√≥n 1: leer los documentos
Opci√≥n 2: vinuestra salida
## Creando el archivo YAML para nuestro conjunto de demonios
Comencemos con el archivo YAML para el rngrecurso actual

Volcar el recurso rng en YAML:

kubectl get deploy/rng -o yaml --export >rng.yml
Editarrng.yml

Nota: --exporteliminar√° la informaci√≥n ‚Äúespec√≠fica del cl√∫ster‚Äù, es decir:

espacio de nombres (para que el recurso no est√© vinculado a un espacio de nombres espec√≠fico) estado y marca de tiempo de creaci√≥n (in√∫til al crear un nuevo recurso) resourceVersion y uid (estos causar√≠an‚Ä¶ problemas interesantes )

## ‚ÄúTransmitir‚Äù un recurso a otro
¬øQu√© pasar√≠a si simplemente cambi√°ramos el kindcampo?

(No puede ser tan f√°cil ¬øverdad?)

Cambiar kind: Deploymentakind: DaemonSet

Guardar, salir

Intenta crear nuestro nuevo recurso:

  kubectl apply -f rng.yml
Todos sab√≠amos que esto no pod√≠a ser tan f√°cil, ¬øverdad?
## Entendiendo el problema
El n√∫cleo del error es:

error validating data:
[ValidationError(DaemonSet.spec):
unknown field "replicas" in io.k8s.api.extensions.v1beta1.DaemonSetSpec,
...
Obviamente , no tiene sentido especificar un n√∫mero de r√©plicas para un conjunto de demonios.

Soluci√≥n alternativa: arregle el YAML

eliminar el replicascampo
eliminar el strategycampo (que define el mecanismo de implementaci√≥n para una implementaci√≥n)
eliminar la status: {}linea al final
O tambi√©n podr√≠amos‚Ä¶

## Utilice el --force, Lucas
Tambi√©n podr√≠amos decirle a Kubernetes que ignore estos errores e intente de todos modos.

El --forcenombre real de la bandera es--validate=false

Intente cargar nuestro archivo YAML e ignore los errores:

kubectl apply -f rng.yml --validate=false
Usa ‚Äìforce, Luke Tambi√©n podr√≠amos decirle a Kubernetes que ignore estos errores e intente de todos modos

El nombre real del indicador ‚Äìforce es ‚Äìvalidate=false

Intenta cargar nuestro archivo YAML e ignora los errores: kubectl apply -f rng.yml ‚Äìvalidate=false üé©‚ú®üêá

Espera‚Ä¶ ¬ø puede ser tan f√°cil?

## Comprobando lo que hemos hecho
¬øHemos transformado nuestro deploymenten un daemonset?

Mire los recursos que tenemos ahora:

kubectl get all
¬°Tenemos ambos deploy/rngy ds/rngahora!

Y una vaina de m√°s‚Ä¶

## Explicaci√≥n
Puede tener diferentes tipos de recursos con el mismo nombre (es decir, una implementaci√≥n y un conjunto de demonios, ambos denominados rng)

Todav√≠a tenemos el antiguo rng despliegue

Pero ahora tambi√©n tenemos el nuevo rng conjunto de demonios.

Si observamos las vainas, tenemos:

una c√°psula para el despliegue
un pod por nodo para el conjunto de demonios
## ¬øQu√© est√°n haciendo todas estas c√°psulas?
Revisemos los registros de todos estos rngpods.

Todas estas c√°psulas tienen una run=rngetiqueta:

la primera vaina, porque eso es lo que kubectl runhace
los otros (en el conjunto de demonios), porque copiamos la especificaci√≥n del primero
Por lo tanto, podemos consultar los registros de todos usando ese run=rngselector.

Verifique los registros de todos los pods que tengan una etiqueta run=rng:

kubectl logs -l run=rng --tail 1
Parece que todos los pods est√°n atendiendo solicitudes en este momento.

## Eliminar el primer pod del balanceador de carga
¬øQu√© pasar√≠a si quit√°ramos esa vaina, con kubectl delete pod ...?

El conjunto de r√©plicas lo recrear√≠a inmediatamente.

¬øQu√© pasar√≠a si quit√°ramos la run=rngetiqueta de esa c√°psula?

Lo replicasetrecrear√≠an inmediatamente.

‚Ä¶Porque lo que importa replicasetes el n√∫mero de pods que coinciden con ese selector .

Pero pero pero‚Ä¶ ¬øNo tenemos m√°s de una c√°psula run=rngahora?

La respuesta est√° en el selector exacto utilizado por replicaset‚Ä¶

## Profundizaci√≥n en los selectores
Veamos los selectores para la rng implementaci√≥n y el conjunto de r√©plicas asociado.

Mostrar informaci√≥n detallada sobre la rngimplementaci√≥n:

kubectl describe deploy rng
Mostrar informaci√≥n detallada sobre la rngr√©plica:

kubectl describe rs rng-yyyy
El selector del conjunto de r√©plicas tambi√©n tiene un pod-template-hash, a diferencia de los pods en nuestro conjunto de demonios.

# Actualizaci√≥n de un servicio mediante etiquetas y selectores
¬øQu√© pasa si queremos eliminar la rngimplementaci√≥n del balanceador de carga?

Opci√≥n 1:

destruyelo
Opcion 2:

## Agregar una etiqueta adicional al conjunto de demonios
Actualice el selector de servicio para hacer referencia a esa etiqueta
Por supuesto, la opci√≥n 2 ofrece m√°s oportunidades de aprendizaje, ¬øverdad?

Agregar una etiqueta adicional al conjunto de demonios
Actualizaremos el conjunto de demonios ‚Äúspec‚Äù

Opci√≥n 1:

Edita el rng.ymlarchivo que usamos anteriormente.
cargar la nueva definici√≥n conkubectl apply
Opcion 2:

usarkubectl edit
Si crees que ya lo has logrado, no dudes en intentarlo directamente. ¬°Hemos incluido algunos consejos en las siguientes diapositivas para tu comodidad!

## Hemos puesto recursos en sus recursos.
Recordatorio: ¬°un conjunto de demonios es un recurso que crea m√°s recursos!

Hay una diferencia entre:

la(s) etiqueta(s) de un recurso (en el metadatabloque al principio)
el selector de un recurso (en el specbloque)
las etiquetas de los recursos creados por el primer recurso (en el templatebloque)
Debe actualizar el selector y la plantilla (las etiquetas de metadatos no son obligatorias)

La plantilla debe coincidir con el selector (es decir, el recurso se negar√° a crear recursos que no seleccione)

## A√±adiendo nuestra etiqueta
Vamos a a√±adir una etiquetaisactive: yes

En YAML, s√≠ debe ir entre comillas; es decirisactive: "yes"

Actualice el conjunto de demonios para agregarlo isactive: "yes"al selector y a la etiqueta de plantilla:

kubectl edit daemonset rng
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: rng      
      isactive: "yes"
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: rng      
        isactive: "yes"
Actualice el servicio para agregarlo isactive: "yes"a su selector:

kubectl edit service rng
## Comprobando lo que hemos hecho
Verifique los registros de todos run=rnglos pods para confirmar que solo 2 de ellos est√©n activos ahora:

kubectl logs -l run=rng
Las marcas de tiempo deber√≠an darnos una pista sobre cu√°ntos pods est√°n recibiendo tr√°fico actualmente.

Mira las vainas que tenemos ahora mismo:

kubectl get pods
## ¬øM√°s etiquetas, m√°s selectores, m√°s problemas?
Ejercicio extra 1: limpia los pods del conjunto de demonios ‚Äúantiguo‚Äù

Ejercicio extra 2: ¬øC√≥mo podr√≠amos haber hecho esto para evitar la creaci√≥n de nuevos pods?

## Actualizaciones continuas
De forma predeterminada (sin actualizaciones continuas), cuando se actualiza un recurso escalado:

Se crean nuevos pods
Las vainas antiguas se terminan
‚Ä¶ todo al mismo tiempo
Si algo sale mal, ¬Ø_(„ÉÑ)_/¬Ø
Con actualizaciones continuas, cuando se actualiza un recurso, esto sucede progresivamente.

Dos par√°metros determinan el ritmo del despliegue: maxUnavailableymaxSurge

Se pueden especificar en n√∫mero absoluto de pods o en porcentaje del replicasrecuento .

En cualquier momento dado ‚Ä¶

Siempre habr√° al menos replicas- maxUnavailablevainas disponibles
Nunca habr√° m√°s de replicas+ maxSurgepods en total
Por lo tanto, habr√° hasta maxUnavailable+ maxSurgepods que se actualizar√°n
Tenemos la posibilidad de volver a la versi√≥n anterior (si la actualizaci√≥n falla o no es satisfactoria de alguna manera)

## Actualizaciones continuas en la pr√°ctica
A partir de Kubernetes 1.8, podemos realizar actualizaciones continuas con:

deployments, daemonsets,statefulsets

La edici√≥n de uno de estos recursos dar√° como resultado autom√°ticamente una actualizaci√≥n continua.

Las actualizaciones continuas se pueden monitorear con el kubectl rolloutsubcomando

## Construyendo una nueva versi√≥n del workerservicio
Editar dockercoins/worker/worker.py, actualizar la l√≠nea de sue√±o para dormir 1 segundo

Ir al directorio de la pila:

cd stacks
Crea una nueva etiqueta y env√≠ala al registro:

  export TAG=v0.2
  docker-compose -f dockercoins.yml build
  docker-compose -f dockercoins.yml push
## Implementaci√≥n del nuevo servicio para trabajadores
Monitoreemos lo que sucede abriendo algunas terminales y ejecutemos:

  kubectl get pods -w
  kubectl get replicasets -w
  kubectl get deployments -w
Actualice el trabajador con kubectl edit o ejecutando:

kubectl set image deploy worker worker=$USERNAME/worker:$TAG
Ese lanzamiento deber√≠a ser bastante r√°pido. ¬øQu√© se muestra en la interfaz web?

## Desplegando un error
¬øQu√© pasa si cometemos un error?

Actualizar el trabajador especificando una imagen inexistente:

export TAG=v0.3
kubectl set image deploy worker worker=$REGISTRY/worker:$TAG
Comprueba lo que est√° pasando:

kubectl rollout status deploy worker
Nuestro lanzamiento est√° estancado. Sin embargo, la aplicaci√≥n no est√° inactiva (solo es un 10 % m√°s lenta).

## C√≥mo recuperarse de un mal lanzamiento
Podr√≠amos enviar alguna imagen v0.3 (la l√≥gica de reintento del pod eventualmente la detectar√° y la implementaci√≥n continuar√°)

O podr√≠amos invocar una reversi√≥n manual.

Cancela la implementaci√≥n y espera a que se calme la situaci√≥n:

kubectl rollout undo deploy worker
kubectl rollout status deploy worker
## Cambiar los par√°metros de implementaci√≥n
Queremos:

volver a v0.1(que ahora nos damos cuenta de que no etiquetamos - ¬°vaya!)
Sea conservador en cuanto a la disponibilidad (tenga siempre el n√∫mero deseado de trabajadores disponibles)
Sea agresivo en la velocidad de implementaci√≥n (actualice m√°s de un pod a la vez)
Dar algo de tiempo a nuestros trabajadores para que se ‚Äúcalienten‚Äù antes de empezar m√°s
Los cambios correspondientes se pueden expresar en el siguiente fragmento de YAML:

spec:
  template:
    spec:
      containers:
      - name: worker
        image: $USERNAME/worker:latest
  strategy:
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 3
  minReadySeconds: 10
## Aplicar cambios a trav√©s de un parche YAML
Podr√≠amos utilizarkubectl edit deployment worker

Pero tambi√©n podr√≠amos usarlo kubectl patchcon el YAML exacto que se mostr√≥ antes.

Aplicamos todos nuestros cambios y esperamos a que surtan efecto:

kubectl patch deployment worker -p "
spec:
  template:
    spec:
      containers:
      - name: worker
        image: $USERNAME/worker:latest
  strategy:
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 3
  minReadySeconds: 10
"
kubectl rollout status deployment worker
## Pr√≥ximos pasos
Bien, ¬øc√≥mo puedo empezar a contenerizar mis aplicaciones?

Lista de verificaci√≥n de contenedorizaci√≥n sugerida:

Escribe un Dockerfile para un servicio en una aplicaci√≥n.
Escribe Dockerfiles para los dem√°s servicios (construibles)
Escribe un archivo Compose para toda esa aplicaci√≥n.
Aseg√∫rese de que los desarrolladores est√©n capacitados para ejecutar la aplicaci√≥n en contenedores.
Configurar compilaciones automatizadas de im√°genes de contenedores desde el repositorio de c√≥digo.
Configurar una canalizaci√≥n de CI utilizando estas im√°genes de contenedor
Configurar una canalizaci√≥n de CD (para pruebas/control de calidad) utilizando estas im√°genes
¬°Y ahora es el momento de mirar la orquestaci√≥n!

## Espacios de nombres
Los espacios de nombres le permiten ejecutar m√∫ltiples pilas id√©nticas una al lado de la otra

Dos espacios de nombres ( por ejemplo blue , y green) pueden tener cada uno su propio redisservicio

Cada uno de los dos redisservicios tiene su propioClusterIP

kube-dnscrea dos entradas, que se asignan a estas dos ClusterIPdirecciones:

redis.blue.svc.cluster.localyredis.green.svc.cluster.local

Los pods en el blueespacio de nombres obtienen un sufijo de b√∫squeda deblue.svc.cluster.local

Como resultado, la resoluci√≥n redisdesde un pod en el blueespacio de nombres produce el resultado ‚Äúlocal‚Äù.redis

Esto no proporciona aislamiento . Eso ser√≠a tarea de las pol√≠ticas de red.

## Servicios con estado (bases de datos, etc.)
Como primer paso, es m√°s prudente mantener los servicios con estado fuera del cl√∫ster.

La exposici√≥n de los mismos a las vainas se puede realizar con m√∫ltiples soluciones:

ExternalNameservicios ( redis.blue.svc.cluster.localquedar√° CNAMEconstancia)

ClusterIPservicios con expl√≠cito Endpoints(en lugar de dejar que Kubernetes genere los puntos finales desde un selector)

Servicios de embajador (proxies a nivel de aplicaci√≥n que pueden proporcionar inyecci√≥n de credenciales y m√°s)

## Servicios con estado (segunda toma)
Si realmente desea alojar servicios con estado en Kubernetes, puede consultar lo siguiente:

vol√∫menes (para transportar datos persistentes)
complementos de almacenamiento
Reclamos de volumen persistente (para solicitar caracter√≠sticas de volumen espec√≠ficas)
Conjuntos con estado (pods que no son ef√≠meros)
## Manejo del tr√°fico HTTP
Los servicios son construcciones de capa 4

HTTP es un protocolo de capa 7

Se maneja mediante entradas (un tipo de recurso diferente)

Los ingresos permiten:

enrutamiento de host virtual
adherencia de la sesi√≥n
Mapeo de URI
¬°y mucho m√°s!
## Registro y m√©tricas
El registro se delega al motor del contenedor

Las m√©tricas generalmente se manejan con Prometheus

## Gestionar la configuraci√≥n de nuestras aplicaciones
Dos construcciones son particularmente √∫tiles: secretos y mapas de configuraci√≥n.

Permiten exponer informaci√≥n arbitraria a nuestros contenedores

Evite almacenar la configuraci√≥n en im√°genes de contenedores (hay algunas excepciones a esa regla, pero generalmente es una mala idea)

Nunca guarde informaci√≥n confidencial en im√°genes de contenedores (es el equivalente en contenedores de la contrase√±a en una nota adhesiva en su pantalla)

## Federaci√≥n de cl√∫steres
La operaci√≥n maestra de Kubernetes se basa en etcd

etcd utiliza el protocolo Raft

Raft recomienda baja latencia entre nodos

¬øQu√© pasa si nuestro cl√∫ster se propaga a m√∫ltiples regiones?

Desglosarlo en cl√∫steres locales

Reagr√∫pelos en una federaci√≥n de cl√∫steres

Sincronizar recursos entre cl√∫steres

Descubra recursos en todos los cl√∫steres

## Experiencia de desarrollador
¬°Lo he puesto al final pero es muy importante!

¬øC√≥mo incorporar a un nuevo desarrollador?

¬øQu√© necesitan instalar para obtener un stack de desarrollo?
¬øC√≥mo pasa un cambio de c√≥digo del desarrollo a la producci√≥n?
¬øC√≥mo agrega alguien un componente a una pila?